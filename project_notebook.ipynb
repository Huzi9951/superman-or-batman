{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Huzi9951/superman-or-batman/blob/main/project_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSjzV-p3Q0aA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptzViLevQ6G5"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "zip_file_path = 'archive.zip'\n",
        "extraction_path = 'data/'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "  zip_ref.extractall(extraction_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhGkqLOccIkC"
      },
      "outputs": [],
      "source": [
        "parent_directory = \"/content/data\"  # Or any desired path\n",
        "subfolder1_name = \"Superman\"\n",
        "subfolder2_name = \"Batman\"\n",
        "os.makedirs(parent_directory, exist_ok=True)\n",
        "subfolder1_path = os.path.join(parent_directory, subfolder1_name)\n",
        "subfolder2_path = os.path.join(parent_directory, subfolder2_name)\n",
        "os.makedirs(subfolder1_path, exist_ok=True)\n",
        "os.makedirs(subfolder2_path, exist_ok=True)\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3GY0tYveeGk"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# Base dataset directory\n",
        "dataset_path = Path(\"data/Superman or Batman\")\n",
        "\n",
        "# Create subfolders for cleaned data\n",
        "superman_path = dataset_path.parent / \"Superman\"\n",
        "batman_path = dataset_path.parent / \"Batman\"\n",
        "superman_path.mkdir(exist_ok=True)\n",
        "batman_path.mkdir(exist_ok=True)\n",
        "\n",
        "# Loop over all .txt files\n",
        "for txt_file in dataset_path.glob(\"*.txt\"):\n",
        "    # Collect all labels in this file\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        labels = {line.strip().split()[0] for line in f if line.strip()}\n",
        "\n",
        "    # Check image file (assuming same name with .jpg)\n",
        "    image_file = txt_file.with_suffix(\".jpg\")\n",
        "\n",
        "    if \"0\" in labels and \"1\" in labels:\n",
        "        # ❌ Mixed labels → delete both\n",
        "        print(f\"Deleting mixed-label pair: {txt_file.name}\")\n",
        "        txt_file.unlink(missing_ok=True)\n",
        "        image_file.unlink(missing_ok=True)\n",
        "    elif labels == {\"0\"}:\n",
        "        # ✅ Only Superman → move to superman folder\n",
        "        print(f\"Moving {txt_file.name} to Superman folder\")\n",
        "        shutil.move(txt_file, superman_path / txt_file.name)\n",
        "        if image_file.exists():\n",
        "            shutil.move(image_file, superman_path / image_file.name)\n",
        "    elif labels == {\"1\"}:\n",
        "        # ✅ Only Batman → move to batman folder\n",
        "        print(f\"Moving {txt_file.name} to Batman folder\")\n",
        "        shutil.move(txt_file, batman_path / txt_file.name)\n",
        "        if image_file.exists():\n",
        "            shutil.move(image_file, batman_path / image_file.name)\n",
        "    else:\n",
        "        print(f\"Unknown label format in {txt_file.name}: {labels}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmWv9-DDffo_"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Define paths to the subfolders\n",
        "base_path = Path(\"data/\")\n",
        "superman_path = base_path / \"Superman\"\n",
        "batman_path = base_path / \"Batman\"\n",
        "\n",
        "# Function to delete all .txt files in a folder\n",
        "def delete_txt_files(folder_path):\n",
        "    for txt_file in folder_path.glob(\"*.txt\"):\n",
        "        print(f\"Deleting: {txt_file}\")\n",
        "        txt_file.unlink()\n",
        "\n",
        "# Delete .txt files from both folders\n",
        "delete_txt_files(superman_path)\n",
        "delete_txt_files(batman_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egE3kuvjgExu"
      },
      "outputs": [],
      "source": [
        "def walk_through_dir(dir_path):\n",
        "  #Walk through dir_path returning its content\n",
        "  for dirpath,dirnames,filenames in os.walk(dir_path):\n",
        "    print(f\"tere are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRSufK2xgK37"
      },
      "outputs": [],
      "source": [
        "walk_through_dir('/content/data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HggeeExnggc3"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Replace with your dataset root if different\n",
        "checkpoint_dir = Path(\"data/.ipynb_checkpoints\")\n",
        "\n",
        "if checkpoint_dir.exists():\n",
        "    shutil.rmtree(checkpoint_dir)\n",
        "    print(\"Deleted .ipynb_checkpoints folder.\")\n",
        "else:\n",
        "    print(\"No .ipynb_checkpoints folder found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ds-8cxpOhVdL"
      },
      "outputs": [],
      "source": [
        "#train and test split\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Base directories\n",
        "original_data_dir = Path(\"/content/data\")\n",
        "new_base_dir = original_data_dir  # reuse the base\n",
        "\n",
        "# Classes\n",
        "classes = [\"Superman\", \"Batman\"]\n",
        "\n",
        "# Create train/test directories\n",
        "for split in [\"train\", \"test\"]:\n",
        "    for cls in classes:\n",
        "        Path(new_base_dir / split / cls).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Function to split and move images\n",
        "def split_data(class_name):\n",
        "    src_dir = original_data_dir / class_name\n",
        "    all_images = [f for f in os.listdir(src_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    train_imgs, test_imgs = train_test_split(all_images, test_size=0.2, random_state=42)\n",
        "\n",
        "    for img in train_imgs:\n",
        "        shutil.copy(src_dir / img, new_base_dir / \"train\" / class_name / img)\n",
        "    for img in test_imgs:\n",
        "        shutil.copy(src_dir / img, new_base_dir / \"test\" / class_name / img)\n",
        "\n",
        "    print(f\"{class_name}: {len(train_imgs)} train, {len(test_imgs)} test\")\n",
        "\n",
        "# Apply to both classes\n",
        "for cls in classes:\n",
        "    split_data(cls)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WkPaRkUh9Zo"
      },
      "outputs": [],
      "source": [
        "image_path=\"data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXuiNKKPh6mG"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "image_path = Path(image_path)\n",
        "\n",
        "train_dir=image_path/ \"train\"\n",
        "test_dir=image_path/ \"test\"\n",
        "test_dir,train_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiIe-rQFiYp4"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "image_path_list=list(image_path.glob('*/*/*.jpg'))\n",
        "random_image_path = random.choice(image_path_list)\n",
        "print(random_image_path)\n",
        "image_class=random_image_path.parent.name\n",
        "print(image_class)\n",
        "img = Image.open(random_image_path)\n",
        "img.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UY74h9AtPFcf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "img_as_array=np.asarray(img)\n",
        "plt.imshow(img)\n",
        "plt.axis(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdK23HArQV-l"
      },
      "outputs": [],
      "source": [
        "img_as_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x_dCTVKQhJo"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYBkKCD-RnaS"
      },
      "outputs": [],
      "source": [
        "transformer=transforms.Compose([\n",
        "    transforms.Resize(size=(128,128)),\n",
        "    transforms.AugMix(),\n",
        "    transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqiBrNlXVI9P"
      },
      "outputs": [],
      "source": [
        "data_transform=transformer(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHZEvCS7VY0W"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img)\n",
        "plt.axis(False)\n",
        "plt.title(image_class)\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(data_transform.permute(1,2,0))\n",
        "plt.axis(False)\n",
        "plt.title(image_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz51ARWxZdPf"
      },
      "outputs": [],
      "source": [
        "def plot_transformed_images(image_paths: list,transform,n=3,seed=None):\n",
        "  #selects random images from a path of images and\n",
        "  #loads/transforms them then plots og vs transformed plot\n",
        "  if seed:\n",
        "    random.seed(seed)\n",
        "  random_image_paths = random.sample(image_paths,k=n)\n",
        "  for image_path in random_image_paths:\n",
        "    with Image.open(image_path) as f:\n",
        "      fig,ax=plt.subplots(nrows=1,ncols=2)\n",
        "      ax[0].imshow(f)\n",
        "      ax[0].set_title(f\"original\\nSize: {f.size}\")\n",
        "      ax[0].axis(False)\n",
        "      #transform and plot target image\n",
        "      transformed_image=transform(f).permute(1,2,0)\n",
        "      ax[1].imshow(transformed_image)\n",
        "      ax[1].set_title(f\"transformed\\nshape: {transformed_image.shape}\")\n",
        "      ax[1].axis(False)\n",
        "\n",
        "      fig.suptitle(f\"class: {image_path.parent.stem}\",fontsize=16)\n",
        "\n",
        "plot_transformed_images(image_paths=image_path_list,transform=transformer,\n",
        "                        n=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swSb2fi-Zl9Z"
      },
      "outputs": [],
      "source": [
        "train_dataset = datasets.ImageFolder(root=train_dir,\n",
        "                                     transform=transformer,\n",
        "                                     target_transform=None)\n",
        "test_dataset = datasets.ImageFolder(root=test_dir,\n",
        "                                     transform=transformer,\n",
        "                                     target_transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfZBo1GzcpiX"
      },
      "outputs": [],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ije9foJeY_O"
      },
      "outputs": [],
      "source": [
        "class_name=train_dataset.classes\n",
        "class_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2MRwocnep3_"
      },
      "outputs": [],
      "source": [
        "class_to_idx=train_dataset.class_to_idx\n",
        "class_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAn-FwyTfPGI"
      },
      "outputs": [],
      "source": [
        "img,label=train_dataset[7]\n",
        "plt.imshow(img.permute(1,2,0))\n",
        "plt.title(class_name[label])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cpAXRXngQYR"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE=1\n",
        "train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True)\n",
        "test_dataloader = DataLoader(dataset=test_dataset,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False)\n",
        "len(train_dataloader),len(test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w40fkPvohFKU"
      },
      "outputs": [],
      "source": [
        "img,label=next(iter(train_dataloader))\n",
        "img.shape,label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eafx0UqliBUq"
      },
      "outputs": [],
      "source": [
        "class VGG(nn.Module):\n",
        "  def __init__(self,input_shape: int,hidden_units: int, output_layer: int):\n",
        "    super().__init__()\n",
        "    self.block_1=nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_shape,out_channels=hidden_units,\n",
        "                  kernel_size=3,stride=1,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,out_channels=hidden_units,\n",
        "                  kernel_size=3,stride=1,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.block_2=nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units,out_channels=hidden_units,\n",
        "                  kernel_size=3,stride=1,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,out_channels=hidden_units,\n",
        "                  kernel_size=3,stride=1,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.block_3=nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units,out_channels=hidden_units,\n",
        "                  kernel_size=3,stride=1,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,out_channels=hidden_units,\n",
        "                  kernel_size=3,stride=1,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.classifier=nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=hidden_units*256,out_features=output_layer)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    x=self.block_1(x)\n",
        "    x=self.block_2(x)\n",
        "    x=self.block_3(x)\n",
        "    x=self.classifier(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FX_4JXtsoXj"
      },
      "outputs": [],
      "source": [
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ooo00vDrmme"
      },
      "outputs": [],
      "source": [
        "model_0=VGG(input_shape=3,hidden_units=30,output_layer=len(train_dataset.classes)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTWWMZAtuQIs"
      },
      "outputs": [],
      "source": [
        "loss_fn=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.Adam(params=model_0.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BYj223aoLNPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8qOTIA0unox"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy0MXp0iuqv_"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "summary(model=model_0,input_size=(1,3,128,128))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCand5aWvTbe"
      },
      "outputs": [],
      "source": [
        "def train_step(model:torch.nn.Module,DataLoader:torch.utils.data.DataLoader,\n",
        "               loss_fn:torch.nn.Module,optimizer: torch.optim.Optimizer,\n",
        "               device=device):\n",
        "  total_loss,acc_score=0,0\n",
        "  model.train()\n",
        "  for batch,(X,y) in enumerate(DataLoader):\n",
        "    X,y=X.to(device),y.to(device)\n",
        "    y_logits=model(X)\n",
        "    loss=loss_fn(y_logits,y)\n",
        "    total_loss+=loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    y_pred_class = torch.argmax(torch.softmax(y_logits, dim=1), dim=1)\n",
        "    acc_score+=(y_pred_class == y).sum().item()/len(y_logits)\n",
        "  total_loss/=len(DataLoader)\n",
        "  acc_score/=len(DataLoader)\n",
        "  return total_loss,acc_score\n",
        "def test_step(model:torch.nn.Module,\n",
        "              dataloader:torch.utils.data.DataLoader,\n",
        "              loss_fn:torch.nn.Module,\n",
        "              device=device):\n",
        "  model.eval()\n",
        "  test_loss,test_acc=0,0\n",
        "  with torch.inference_mode(): # Call torch.inference_mode as a function\n",
        "    for batch,(X,y) in enumerate(dataloader):\n",
        "      X,y=X.to(device),y.to(device)\n",
        "      y_logits=model(X)\n",
        "      loss=loss_fn(y_logits,y)\n",
        "      test_loss+=loss.item()\n",
        "      y_pred_class = torch.argmax(torch.softmax(y_logits, dim=1), dim=1)\n",
        "      test_acc+=(y_pred_class == y).sum().item()/len(y_logits) # Corrected variable name\n",
        "    test_loss/=len(dataloader)\n",
        "    test_acc/=len(dataloader) # Corrected variable name\n",
        "    return test_loss,test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFZxPMR1vrVc"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxI0nqXTvsop"
      },
      "outputs": [],
      "source": [
        "def train_test_loop(epoch:int,model:nn.Module,\n",
        "                    train_dataloader:torch.utils.data.DataLoader,\n",
        "                    test_dataloader:torch.utils.data.DataLoader,\n",
        "                    loss_fn:torch.nn.Module,optimizer=torch.optim.Optimizer,\n",
        "                    device=device):\n",
        "  result={'train_loss':[],\n",
        "          'train_acc':[],\n",
        "          'test_loss':[],\n",
        "          'test_acc':[]}\n",
        "  for epoch in tqdm(range(epoch)):\n",
        "    print(f\"epoch: {epoch}\")\n",
        "    train_loss,train_acc=train_step(model=model,DataLoader=train_dataloader,\n",
        "               loss_fn=loss_fn,optimizer=optimizer,device=device)\n",
        "    test_loss,test_acc=test_step(model=model,dataloader=test_dataloader,\n",
        "              loss_fn=loss_fn,device=device)\n",
        "\n",
        "    print(f\"epoch:{epoch}|train_acc:{train_acc*100:.4f}|train_loss:{train_loss:.4f}|test_acc:{test_acc*100:.4f}|test_loss:{test_loss:.4f}\")\n",
        "    result[\"train_loss\"].append(train_loss)\n",
        "    result[\"train_acc\"].append(train_acc)\n",
        "    result[\"test_loss\"].append(test_loss)\n",
        "    result[\"test_acc\"].append(test_acc)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfOQGFfqvxEh"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "NUM_EPOCHS=70\n",
        "from timeit import default_timer as timer\n",
        "start_time=timer()\n",
        "model_0_results=train_test_loop(epoch=NUM_EPOCHS,model=model_0,\n",
        "                train_dataloader=train_dataloader,\n",
        "                test_dataloader=test_dataloader,\n",
        "                loss_fn=loss_fn,optimizer=optimizer,\n",
        "                device=device)\n",
        "stop_timer=timer()\n",
        "print(f\"time taken: {stop_timer-start_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNWlD7FLxWJk"
      },
      "outputs": [],
      "source": [
        "torch.save(model_0.state_dict(), 'model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"model.pth\")\n"
      ],
      "metadata": {
        "id": "lYkvWYmIFq-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wQwyrEvaFxZR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPVAtz74iar759tpBPv5gle",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}